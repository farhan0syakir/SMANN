# Config format schema number
format_version: 4
# data_path: "../../data"
data_path: "../input/lyft-motion-prediction-autonomous-vehicles"
###################
## Model options
model_params:
  history_num_frames: 20
  history_step_size: 1
  history_delta_time: 0.1
  future_num_frames: 40
  future_step_size: 1
  future_delta_time: 0.1
  ae_hidden_size: 48
  num_modes: 10
  model_name: "Mantra02"
  final_decoder: "StackedTransformerDecoder" #MantraLSTMDecoder, MantraDecoder, TransformerDecoder, StackedTransformerDecoder
  lr: 0.00001
  step_time: 0.1
  topk: 3
  render_ego_history: True

memory_path: "../output/Kitti/memory_3.pth"

input:
  ae_last_iter: 1030
  ae_past_encoder_path: "../output/Kitti/autoencoder/1030/past_encoder.pth"
  ae_future_encoder_path: "../output/Kitti/autoencoder/1030/future_encoder.pth"
  ae_future_decoder_path: "../output/Kitti/autoencoder/1030/future_decoder.pth"
  memory_controller_last_iter: 50
  memory_controller_path: "../output/Kitti/memory_controller/50.pth"
  final_decoder_last_iter: 0
  map_encoder_path: ""
  final_decoder_path: ""
#  final_decoder_path: "../output/Kitti/final_decoder/10.pth"
  baseline_last_iter: 0
  baseline_path: 'output/Rocket/387000_baseline.pth'
  resnet18_with_memory_last_iter: 0
  resnet18_with_memory_path: ''


output:
  baseline_output_folder: 'output/Mantra02/baseline'
  ae_output_folder: '../output/Kitti/autoencoder'
  memory_controller_output_folder: '../output/Kitti/memory_controller'
  map_encoder_output_folder: ''
  final_decoder_output_folder: '../output/Kitti/transformer_top10/final_decoder'
  resnet18_with_memory_folder: 'output/Rocket/memory_resnet18'

###################
## Input raster parameters
raster_params:
  # raster image size [pixels]
  raster_size:
    - 224
    - 224
  # raster's spatial resolution [meters per pixel]: the size in the real world one pixel corresponds to.
  pixel_size:
    - 0.5
    - 0.5
  # From 0 to 1 per axis, [0.5,0.5] would show the ego centered in the image.
  ego_center:
    - 0.25
    - 0.5
  map_type: "py_semantic"

  # the keys are relative to the dataset environment variable
  satellite_map_key: "aerial_map/aerial_map.png"
  semantic_map_key: "semantic_map/semantic_map.pb"
  dataset_meta_key: "meta.json"

  # e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being
  # one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.
  filter_agents_threshold: 0.5

  # whether to completely disable traffic light faces in the semantic rasterizer
  disable_traffic_light_faces: False

  # When set to True, the rasterizer will set the raster origin at bottom left,
  # i.e. vehicles are driving on the right side of the road.
  # With this change, the vertical flipping on the raster used in the visualization code is no longer needed.
  # Set it to False for models trained before v1.1.0-25-g3c517f0 (December 2020).
  # In that case visualisation will be flipped (we've removed the flip there) but the model's input will be correct.
  set_origin_to_bottom: True

###################
## Data loader options
train_data_loader:
  key: "scenes/train.zarr"
#  key: "scenes/sample.zarr"
  batch_size: 12
  shuffle: False
  num_workers: 0

test_data_loader:
#  key: "scenes/sample.zarr"
  key: 'scenes/test.zarr'
  # key: "scenes/validate_chopped_100/validate.zarr"
  batch_size: 12
  shuffle: False
  num_workers: 0

val_data_loader:
  key: "scenes/sample.zarr"
#  key: "scenes/validate.zarr"
  # key: "scenes/validate_chopped_100/validate.zarr"
  batch_size: 12
  shuffle: True
  num_workers: 0

###################
## Train params
train_params:
  # checkpoint_every_n_steps: 1000
  # max_num_steps: 150000
  checkpoint_every_n_steps: 10
  max_num_steps: 10000
  eval_every_n_steps: 1000

val_params:
  max_num_steps: 5000